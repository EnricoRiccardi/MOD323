{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Packages, numpy,matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    \"\"\"GridWorld environment with obstacles and a goal.\n",
    "    The agent starts at the top-left corner and has to reach the bottom-right corner.\n",
    "    The agent receives a reward of -1 at each step, a reward of -0.01 at each step in an obstacle, \n",
    "    and a reward of 1 at the goal.\n",
    "    \n",
    "    Args:\n",
    "        size (int): The size of the grid.\n",
    "        num_obstacles (int): The number of obstacles in the grid.\n",
    "        \n",
    "    Attributes:\n",
    "        size (int): The size of the grid.\n",
    "        num_obstacles (int): The number of obstacles in the grid.\n",
    "        obstacles (list): The list of obstacles in the grid.\n",
    "        state_space (numpy.ndarray): The state space of the grid.\n",
    "        state (tuple): The current state of the agent.\n",
    "        goal (tuple): The goal state of the agent.\n",
    "    \n",
    "    Methods:\n",
    "        generate_obstacles: Generate the obstacles in the grid.\n",
    "        step: Take a step in the environment.\n",
    "        reset: Reset the environment.\n",
    "    \"\"\"\n",
    "    def __init__(self, size=5):\n",
    "        self.size = size\n",
    "                \n",
    "        self.state_space = np.zeros((self.size, self.size))\n",
    "        self.state = (0, 0)\n",
    "        self.goal = (self.size-1, self.size-1)\n",
    "\n",
    "        self.obstacles = [(0, 4), (4, 3), (1, 3), (1, 0), (3, 2)]\n",
    "        \n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take a step in the environment.\n",
    "        The agent takes a step in the environment based on the action it chooses.\n",
    "\n",
    "        Args:\n",
    "            action (int): The action the agent takes.\n",
    "                0: up\n",
    "                1: right\n",
    "                2: down\n",
    "                3: left\n",
    "        \n",
    "        Returns:\n",
    "            state (tuple): The new state of the agent.\n",
    "            reward (float): The reward the agent receives.\n",
    "            done (bool): Whether the episode is done or not.\n",
    "        \"\"\"\n",
    "        x, y = self.state\n",
    "        if action == 0:  # up\n",
    "            x = max(0, x-1)\n",
    "        elif action == 1:  # right\n",
    "            y = min(self.size-1, y+1)\n",
    "        elif action == 2:  # down\n",
    "            x = min(self.size-1, x+1)\n",
    "        elif action == 3:  # left\n",
    "            y = max(0, y-1)\n",
    "        self.state = (x, y)\n",
    "        if self.state in self.obstacles:\n",
    "            return self.state, -1, True\n",
    "        if self.state == self.goal:\n",
    "            return self.state, 1, True\n",
    "        return self.state, -0.1, False\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment.\n",
    "        The agent is placed back at the top-left corner of the grid.\n",
    "\n",
    "        Args:\n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            state (tuple): The new state of the agent.\n",
    "        \"\"\"\n",
    "        self.state = (0, 0)\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning - How to Train Agent to Learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning:\n",
    "    \"\"\"\n",
    "    Q-Learning agent for the GridWorld environment.\n",
    "\n",
    "    Args:\n",
    "        env (GridWorld): The GridWorld environment.\n",
    "        alpha (float): The learning rate.\n",
    "        gamma (float): The discount factor.\n",
    "        epsilon (float): The exploration rate.\n",
    "        episodes (int): The number of episodes to train the agent.\n",
    "    \n",
    "    Attributes:\n",
    "        env (GridWorld): The GridWorld environment.\n",
    "        alpha (float): The learning rate.\n",
    "        gamma (float): The discount factor.\n",
    "        epsilon (float): The exploration rate.\n",
    "        episodes (int): The number of episodes to train the agent.\n",
    "        q_table (numpy.ndarray): The Q-table for the agent.\n",
    "    \n",
    "    Methods:\n",
    "        choose_action: Choose an action for the agent to take.\n",
    "        update_q_table: Update the Q-table based on the agent's experience.\n",
    "        train: Train the agent in the environment.\n",
    "        save_q_table: Save the Q-table to a file.\n",
    "        load_q_table: Load the Q-table from a file.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, alpha=0.5, gamma=0.95, epsilon=0.1, episodes=10):\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.episodes = episodes\n",
    "        self.q_table = np.zeros((self.env.size, self.env.size, 4))\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"\n",
    "        Choose an action for the agent to take.\n",
    "        The agent chooses an action based on the epsilon-greedy policy.\n",
    "        \n",
    "        Args:\n",
    "            state (tuple): The current state of the agent.\n",
    "        \n",
    "        Returns:\n",
    "            action (int): The action the agent takes.\n",
    "                0: up\n",
    "                1: right\n",
    "                2: down\n",
    "                3: left\n",
    "        \"\"\"\n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            return np.random.choice([0, 1, 2, 3])  # exploration\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])  # exploitation\n",
    "\n",
    "    def update_q_table(self, state, action, reward, new_state):\n",
    "        \"\"\"\n",
    "        Update the Q-table based on the agent's experience.\n",
    "        The Q-table is updated based on the Q-learning update rule.\n",
    "\n",
    "        Args:\n",
    "            state (tuple): The current state of the agent.\n",
    "            action (int): The action the agent takes.\n",
    "            reward (float): The reward the agent receives.\n",
    "            new_state (tuple): The new state of the agent.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        ## Q-Learning update rule (to be implemented by the student)\n",
    "\n",
    "        \n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the agent in the environment.\n",
    "        The agent is trained in the environment for a number of episodes.\n",
    "        The agent's experience is stored and returned.\n",
    "\n",
    "        Args:\n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            rewards (list): The rewards the agent receives at each step.\n",
    "            states (list): The states the agent visits at each step.\n",
    "            starts (list): The start of each new episode.\n",
    "            steps_per_episode (list): The number of steps the agent takes in each episode.\n",
    "        \"\"\"\n",
    "        rewards = []\n",
    "        states = []  # Store states at each step\n",
    "        starts = []  # Store the start of each new episode\n",
    "        steps_per_episode = []  # Store the number of steps per episode\n",
    "        steps = 0  # Initialize the step counter outside the episode loop\n",
    "        episode = 0\n",
    "        #print(self.q_table)\n",
    "        while episode < self.episodes:\n",
    "            state = self.env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                action = self.choose_action(state)\n",
    "                new_state, reward, done = self.env.step(action)\n",
    "                self.update_q_table(state, action, reward, new_state)\n",
    "                state = new_state\n",
    "                total_reward += reward\n",
    "                states.append(state)  # Store state\n",
    "                steps += 1  # Increment the step counter\n",
    "                if done and state == self.env.goal:  # Check if the agent has reached the goal\n",
    "                    starts.append(len(states))  # Store the start of the new episode\n",
    "                    rewards.append(total_reward)\n",
    "                    steps_per_episode.append(steps)  # Store the number of steps for this episode\n",
    "                    steps = 0  # Reset the step counter\n",
    "                    episode += 1\n",
    "        return rewards, states, starts, steps_per_episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run and Visualize the Agent & Environment Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld(size=5)\n",
    "agent = QLearning(env)\n",
    "\n",
    "\n",
    "rewards, states, starts, steps_per_episode = agent.train()  # Get starts and steps_per_episode as well\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.rcParams['figure.dpi'] = 200  \n",
    "\n",
    "def update(i):\n",
    "        \"\"\"\n",
    "        Update the grid with the agent's movement.\n",
    "        \n",
    "        Args:\n",
    "            i (int): The current step.\n",
    "        \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        ax.clear()\n",
    "        # Calculate the cumulative reward up to the current step\n",
    "        #print(rewards)\n",
    "        cumulative_reward = sum(rewards[:i+1])\n",
    "        #print(rewards[:i+1])\n",
    "        # Find the current episode\n",
    "        current_episode = next((j for j, start in enumerate(starts) if start > i), len(starts)) - 1\n",
    "        # Calculate the number of steps since the start of the current episode\n",
    "        if current_episode < 0:\n",
    "            steps = i + 1\n",
    "        else:\n",
    "            steps = i - starts[current_episode] + 1\n",
    "        ax.set_title(f\"Iteration: {current_episode+1}, Steps: {steps}\")\n",
    "        grid = np.zeros((env.size, env.size))\n",
    "        for obstacle in env.obstacles:\n",
    "            grid[obstacle] = -1\n",
    "        grid[env.goal] = 1\n",
    "        grid[states[i]] = 0.5  # Use states[i] instead of env.state\n",
    "        ax.imshow(grid, cmap='cool')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani = animation.FuncAnimation(fig, update, frames=range(len(states)), repeat=False)\n",
    "\n",
    "for i, steps in enumerate(steps_per_episode, 1):\n",
    "        print(f\"Iteration {i}:\")\n",
    "        print(f\"Steps to Reach Goal: {steps}, Episode's Reward: {rewards[i-1]:.2f}\")\n",
    "\n",
    "        #{steps} (Steps to reach goal) {rewards[i-1]:.2f} reward \")\n",
    "#print(f\"Total reward: {sum(rewards):.2f}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise:\n",
    "\n",
    "- Increase the number of episodes and observe the learning process (when learning stops)\n",
    "- Change the alpha value and see how that affects the learning process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The above cde intitially was in thsi github rep:\n",
    "## https://github.com/cristianleoo/Reinforcement-Learning/blob/main/Turtorial%202%20-%20Q-Learning/main.py\n",
    "## but the original code was chnage to fit the requirements of the class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
