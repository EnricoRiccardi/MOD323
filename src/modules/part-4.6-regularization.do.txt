
!split
======= Regularization =======


Conventionally, MSE is used as loss function:


!bblock
!bt
\[
MSE = \frac{1}{n} \sum_{i=1}^n (y_i -\bar y)^2
\]
!et
!eblock


We might want to force our model not not fit so well the data,
i.e. we *penalise* the model.


!bt
\[
L1\_loss\_regularized = MSE + \lambda \sum_{i=1}^m |b_i|
\]

\[
L2\_loss\_regularized = MSE + \lambda \sum_{i=1}^m b_i^2
\]

\[
Elastic \ Net = MSE + \lambda_1 \sum_{i=1}^m |b_i| + \lambda_2 \sum_{i=1}^m b_i^2
\]
!et



!split
===== Norms =====
FIGURE: [../figures/norm, width=100 frac=1.0]


!split
===== Outcomes =====
FIGURE: [../figures/n65, width=100 frac=1.0]




!split
===== Regularization =====
$\lambda$ is the regularisation strength. The larger value it has, the more the target function is "wrong". For $\lambda=0$, no regularisation is imposed.


\pause


Forcing a model to be "wrong" might sound rather counter-intuitive.
Furthermore, we have introduced one more parameter that has not much meaning?!


\pause
!bblock Bias - variance trade off
!bt
\[
Error = bias^2 + variance + statistical \ error
\]
!et
!eblock


!split
===== Bias =====
Bias is the difference of the average value of predictions ($q$) from the true relationship function ($f$):


!bt
\[
bias[q(x)] = \mathbb{E}[q(x)] - f(x)
\]
!et


Variance is the expectation of the squared deviation of q(x) from its expected value $\mathbb{E}[q(x)]$.
!bt
\[
var[q(x)] = \mathbb{E}[(q(x) - \mathbb{E}[q(x))^2]
\]
!et


!split
===== Bias-Variance =====
FIGURE: [../figures/bias_variance, width=200 frac=1.0]


!split
===== Bias-Variance =====
FIGURE: [../figures/bias_variance_2, width=100 frac=1.0]





