!split
======= Classification types =======

There are two main types of classification methods for analysis of a set
of objects stored in matrix *X*:


!bslidecell 00
\pause
!bblock Unsupervised classification
* Only the *X* data is used
* "Natural" classes/clusters/groupings in *X* are discovered
!eblock
!eslidecell

!bslidecell 01
\pause
!bblock Supervised classification
* We know the class/group/cluster membership of every object/sample
* Class information is stored in an *Y* matrix
!eblock
!eslidecell

!split
===== Classification types =====
Several approaches can be found in classification tasks:

!bslidecell 00
\pause
!bblock Unsupervised classification
* Principal component analysis (PCA)
* Agglomerative (hierarchical) cluster analysis.
* k-means cluster analysis
* Fuzzy c-means cluster analysis
* Self organising feature maps (SOFM)
!eblock
!eslidecell

!bslidecell 01
\pause
!bblock Supervised classification
* Linear discriminant analysis (LDA)
* k-nearest neighbours (kNN)
* Discriminant partial least squares
* Soft independent modelling of class analogies (SIMCA)
* Support vector machine (SVM)
!eblock
!eslidecell


!split
===== Limitations of unsupervised classification =====

* Do "natural" clusters in a data set exist and/or have any meaning?

* First we must have a definition of what is a cluster. To do this we
must define what we mean by \alert{similar} or \alert{dissimilar} objects.

* Objects that are \alert{close} have low dissimilarity and high similarity.

\pause

!bblock
A metric system is required.
!eblock

!split
===== Proximity: Continuous variables =====

!bblock Triangle inequality

Considering a vectors in an N-dimensional space, to be a \alert{distance} it must satisfy the \alert{triangle inequality}:

!eblock

!bt
\begin{displaymath}
d_{ij} + d_{im} \geq d_{jm}
\end{displaymath}
!et

If also $d_{jj}=0$, if $i=j$ and $d_{jj}-d_{ii}=0$, then we call it a {\em metric}.

FIGURE: [../figures/triangleineq.eps, frac=0.3]


!split
===== Proximity: Continuous measures =====

!bblock Common metrics:
* Euclidean.

!bt
\begin{displaymath}
d_{ij}^{(E)} = \left[ \sum_{k=1}^{N} (x_{ik} - x_{jk})^2 \right]^{\frac{1}{2}}
\end{displaymath}
!et

* Manhattan
!bt
\begin{displaymath}
d_{ij}^{(M)} = \sum_{k=1}^{N} \|x_{ik} - x_{jk} \|
\end{displaymath}
!et

* Minkowski
!bt
\begin{displaymath}
d_{ij}^{(M(p))} = \left[ \sum_{k=1}^{N} (x_{ik} - x_{jk})^p \right]^{\frac{1}{p}}
\end{displaymath}
!et
!eblock


!split
===== Proximity: Categorical variables =====

* Many applications consist of binary vectors, typical is "yes"
  and "no" answers to a lot of tests
* It is tempting to use distance between binary vectors to signify
  distance. However that is *by far* not optimal.

\pause

Lets look at an example:

* $ \textbf{v}_1 = [1~1~0~0~0~0]$
* $ \textbf{v}_2 = [0~0~1~1~0~0]$
* $ \textbf{v}_3 = [1~1~1~1~1~1]$

It makes NO SENSE to compute the Euclidean distances between these vectors


!split
===== Proximity: Categorical variables: Binary matching =====

!bt
\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|}  \hline \hline
                   	& {\em Object B} value 1 & {\em Object B}  value 0  \\ \hline
{\em Object A} value 1 &  	a             	&   	b  	\\ \hline
{\em Object A} value 0 &  	c             	&   	d  	\\ \hline \hline
\end{tabular}
\end{center}
\end{table}
!et


!bblock Example

!bslidecell 00
!bt
\begin{eqnarray*}
\textbf{a} = \left[0~0~0~1\right]  \\ \nonumber
\textbf{b} = \left[1~1~0~1\right]  \\ \nonumber
\end{eqnarray*}
!et
!eslidecell

!bslidecell 01
* $c=2$: two places where A has 0 and B has 1.
* $d=1$: one place where A and B are equal to 0.
* $a=1$: one place where A and B are equal to 1.
!eslidecell
!eblock


!split
===== Classification metric =====

FIGURE: [../figures/classification-score-matrix, frac=1.1]

