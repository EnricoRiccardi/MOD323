!split
======= K-NN clustering classification =======
 To initiatve k-NN (nearest neighbours), we can use first k-mean.

 The k-means clustering method is used for unsupervised classification, and it can provide
initial cluster centers. The approach requires to:

!bpop
* Assume the existance of $K$ classes, with $N_j$ objects in each class
* Perform k-means clustering with $L_{k}$ clusters for each class
  data set giving $L_{k}$ cluster centers
* Use $L_{k}$ cluster centers to perform a classification
* The closest objects are inspected and is given 
  membership corresponding to the nearest cluster.
* The centers are recomputed as average position of their members.
!epop


!split
===== k-nearest neighbor classification (k-NN) ===== 

k-NN is an extension of just assigning the class label of the
centroids closest to a new object. In k-NN the $k$ nearest
neighbors of the new object are used in the assignment process. 

The algorithm for kNN can be formulated as follows:

!bblock
* Let $\textbf{x}$ be new object to classify
* FOR {$i = 1:N$}
  * $d_{i} = ||\textbf{X}_{i} - \textbf{x}||$
* ENDFOR
* Find the $k$ nearest neighbors to $\textbf{x}$ by sorting $d_{i}$
* Use majority rule: Class to $\textbf{x}$ is equal to the most frequent class among the $k$ neighbors.
!eblock



!split
===== k-nearest neighbor classification (k-NN) ===== 

FIGURE: [../figures/kNN-1, frac=0.8 width=150]



!split
===== k-nearest neighbor classification (k-NN) =====

!bpop
* kNN does not create a decision boundary model
* The "model" consists of a set of objects with known class
  membership which are always used in the prediction
* It is difficult to extract what are the important attributes for
  the classification
* As no actual mathematical model is created, it is difficult to
  generalise the results. 
* kNN can be cumbersome to use because of having to keep the
  calibration set of object for every prediction. If very large
  feature vectors are used this could be computational/storage
  problem.
!epop


!split
===== k-nearest neighbor classification (k-NN) =====

The main advantages of the method are as follows:

!bpop
* Simple to understand
* Simple to implement
* Can handle complex decision boundaries. Rule of thumb: If k-NN
  can't classify the objects, other methods most likely will also fail.
!epop


!split
===== Validation of k-NN =====

The results will be dependent on the $k$ parameter:

!bslidecell 00
FIGURE: [../figures/knn-ex1, frac=1.0]
!eslidecell
 
!bslidecell 01
FIGURE: [../figures/knn-ex2, frac=1.0]
!eslidecell 

